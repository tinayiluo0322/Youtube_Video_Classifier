{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify YouTube Category Based on Video Tags\n",
    "#### Group member: Tina Yi, Xueqing (Annie) Wu, Jiayi Zhou\n",
    "#### Date: Dec 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to classify the YouTube videos based on the tags of videos (a form of the descriptions for videos that enables users to search for content). It automates the categorization process so that it can improve the efficiency of categorization and potentially help advertisements targeting. \n",
    "\n",
    "The data we’re using is from Kaggle (https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset?select=US_youtube_trending_data.csv). It contains data from multiple countries, and we only selected the video records from the United States. Each row represents the tags for one specific video. One Video only belongs to one category. We’re specifically interested in whether the video belongs to music or sports category, which are the two major categories in the data. Each category has around 30,000 records. \n",
    "\n",
    "The original dataset includes 16 columns and almost 30 categories. We only include the music and sports category. We removed the characters that are not in English characters or numbers to exclude other languages. We also converted the characters into lowercase. We tokenize our data into the form of lists of lists. Each record is a list including word as elements, and the lists of each record concatenate together as one large list. Finally, we stored the tokens of the two categories into separate txt files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "##### Generative Probablistic Model--Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Naive Bayes model as our generative probabilistic model for this classification problem. \n",
    "\n",
    "The formula for Naive Bayes algorithm is argmax(P(d|c)P(c)). The logic behind the model is to maximize the probability of each tag represented in each category. The assumption for this model is that the sequence doesn’t matter and feature probabilities P(d|c) are independent given the class. To avoid extremely small probability, we are adding up log probabilities for each word in each category and the probability of each category. \n",
    "\n",
    "In our model, first, the words are converted into embeddings using raw count and TF-IDF. Then, we train the data by `MultinomialNB()`. \n",
    "\n",
    "The accuracy is calculated based on the percentage of data correctly categorized among all data. Besides, we include a timer to track the time it spends on the training process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discrimitive Neural Network--RNN\n",
    "For the discriminative neural network, we designed a fundamental recurrent neural network model tailored for text classification, specifically aimed at categorizing YouTube videos into two categories based on their tags.\n",
    "\n",
    "In this neural network architecture, the process begins by converting input words into dense vectors through an embedding layer. The `nn.Embedding(input_size, hidden_size)` operation facilitates the mapping of words (represented as indices) to dense vectors of a predetermined size (`hidden_size`). Subsequently, the embedded sequences undergo processing by a recurrent layer. The `nn.RNN(hidden_size, hidden_size, batch_first=True)` layer handles input sequences of embedded vectors, generating output sequences. This layer employs a state of size `hidden_size`, initialized to 64. Following this, the output from the RNN layer is channeled through a linear layer. The `nn.Linear(hidden_size, output_size)` operation maps the RNN output to the final output classes, which are binary—specifically, music or sports.\n",
    "\n",
    "To optimize the model's performance, the output is juxtaposed with the actual labels using the cross-entropy loss. The Adam optimizer is then enlisted to minimize this loss. The training loop embarks on a series of epochs, with the current run comprising 20. Within each epoch, the DataLoader (`dataloader`) facilitates the iteration over batches of data. To ensure effective parameter updates, the optimizer's gradients are reset to zero using `optimizer.zero_grad()`. The model's output for the input batch is computed (`outputs = model(inputs)`), and subsequently, the loss is calculated, and backpropagation is executed (`loss.backward()`). The optimizer then takes a step, updating the model parameters based on the computed gradients (`optimizer.step()`).\n",
    "\n",
    "Throughout the training process, key metrics such as total loss, correct predictions, and total samples are dynamically updated. Accuracy is computed based on the ratio of correct predictions to the total number of samples. After each epoch, the average loss and accuracy for that specific epoch are printed. The overall training time is determined by recording the start and end times of the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "#### Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes Model on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm      | Embedding Method | Training Accuracy | Testing Accuracy | Time   |\n",
    "|-----------------|------------------|-------------------|-------------------|--------|\n",
    "| Naive Bayes     | Raw count        | 96.06%            | 96.02%            | 45.8s  |\n",
    "| Naive Bayes     | TF-IDF           | 96.09%            | 96.05%            | 16.2s  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the real data, the accuracy has been relatively high for the Naive Bayes method. The accuracy is quiet consistent between training and testing dataset, which indicates that the Naive bayes model has good generalization. The TF-IDF embedding method is slightly better than Raw count embedding, as it emphasizes more on those tokens that are more relevant to the video tags. Besides, the time spent by TF-IDF is significantly less than Raw count method as it generates sparser representation of the token matrix. \n",
    "\n",
    "Comparing to RNN model, Naive Bayes model is easier to interpret and to run as it is less complicated. With this dataset, Naive Bayes mostly fulfills the two assumption. First, though there are some phrases that have sequence, most of the tags are single words or short phrases, so it is not hugely bounded by the sequence. Second, the feature probability given each category is independent as each tags represents one video and the tag is independently generated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Naive_Bayes as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts (train): 0.9606124357176251\n",
      "raw_counts (test): 0.9601803155522164\n",
      "Time for raw counts section: 45.80 seconds\n",
      "tfidf (train): 0.9608795832498497\n",
      "tfidf (test): 0.9604808414725771\n",
      "Time for tfidf section: 16.20 seconds\n"
     ]
    }
   ],
   "source": [
    "nb.run_experiment(\"../data/category10.txt\", \"../data/category17.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN Model on Real Data\n",
    "In the qualitative evaluation of the RNN on real data, the loss consistently decreases over epochs, signifying a positive indication that the model is effectively learning. Additionally, the accuracy exhibits an upward trend throughout epochs, suggesting an improvement in the model's proficiency in distinguishing between music and sports. Regarding training time, the duration for each epoch seems reasonable and exhibits consistency.\n",
    "Quantitatively, the final accuracy after 20 epochs is approximately 89.57%, reflecting a commendable outcome. The total training time for 20 epochs stands at around 305.60 seconds, which is deemed acceptable.\n",
    "The RNN is specifically crafted to capture sequential dependencies in data. In this instance, the model adeptly learns temporal patterns within the input sequences of video tags. It also effectively handles variable-length sequences by implementing sequence padding. The achievement of an accuracy rate of approximately 89.57% underscores the model's capability to discern between music and sports categories proficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RNN as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read files done\n",
      "Start training\n",
      "Epoch 1/20, Loss: 0.6743, Accuracy: 0.5874, Time: 15.81 seconds\n",
      "Epoch 2/20, Loss: 0.6448, Accuracy: 0.6015, Time: 15.80 seconds\n",
      "Epoch 3/20, Loss: 0.6349, Accuracy: 0.6153, Time: 15.55 seconds\n",
      "Epoch 4/20, Loss: 0.5541, Accuracy: 0.7402, Time: 15.47 seconds\n",
      "Epoch 5/20, Loss: 0.4600, Accuracy: 0.8186, Time: 15.43 seconds\n",
      "Epoch 6/20, Loss: 0.4448, Accuracy: 0.8132, Time: 15.44 seconds\n",
      "Epoch 7/20, Loss: 0.5575, Accuracy: 0.6869, Time: 15.62 seconds\n",
      "Epoch 8/20, Loss: 0.5817, Accuracy: 0.6696, Time: 15.71 seconds\n",
      "Epoch 9/20, Loss: 0.5624, Accuracy: 0.6874, Time: 15.79 seconds\n",
      "Epoch 10/20, Loss: 0.4873, Accuracy: 0.7559, Time: 15.59 seconds\n",
      "Epoch 11/20, Loss: 0.5451, Accuracy: 0.7085, Time: 14.77 seconds\n",
      "Epoch 12/20, Loss: 0.4229, Accuracy: 0.8368, Time: 14.81 seconds\n",
      "Epoch 13/20, Loss: 0.3827, Accuracy: 0.8666, Time: 14.95 seconds\n",
      "Epoch 14/20, Loss: 0.3800, Accuracy: 0.8361, Time: 14.97 seconds\n",
      "Epoch 15/20, Loss: 0.2767, Accuracy: 0.8842, Time: 14.92 seconds\n",
      "Epoch 16/20, Loss: 0.2797, Accuracy: 0.8846, Time: 15.08 seconds\n",
      "Epoch 17/20, Loss: 0.2947, Accuracy: 0.8869, Time: 15.03 seconds\n",
      "Epoch 18/20, Loss: 0.3576, Accuracy: 0.8783, Time: 14.98 seconds\n",
      "Epoch 19/20, Loss: 0.3463, Accuracy: 0.8873, Time: 14.92 seconds\n",
      "Epoch 20/20, Loss: 0.3265, Accuracy: 0.8957, Time: 14.97 seconds\n",
      "Total training time: 305.60 seconds\n"
     ]
    }
   ],
   "source": [
    "rnn.train_rnn_model(\"../data/category10.txt\", \"../data/category17.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data is generated using the same tokens from the real dataset but with probablistic generative model. We chose unigram model as our probabilistic generative model. What we did essentially was to replace the tokens in the original dataset with a randomly generated token based on the appearance probability within the real dataset. We limited the length of the tokens for synthetic data to 10000 due to the capacity issue. The sequence of the synthetic data is random, though there were only little sequence in the real data (mostly single words in the tags, sometimes phrases). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes on Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm      | Embedding Method | Training Accuracy | Testing Accuracy | Time   |\n",
    "|-----------------|------------------|-------------------|-------------------|--------|\n",
    "| Naive Bayes     | Raw count        | 98.79%            | 98.95%            | 13.52s |\n",
    "| Naive Bayes     | TF-IDF           | 98.78%            | 98.95%            | 4.31s  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result accuracy for Naive Bayes algorithm has increases from around 96% to almost 99%. The accuracy is stable between the training and testing dataset, which indicates good generation of model. A possible reason that the accuracy has improved with synthetic data is because the synthetic data is already generated with probabilistic model. Though the synthetic data is generated using the probability of each token in the real dataset, the process of generating the word is randomly selected according to the probability of each token in the real dataset. Hence, the tokens are not exactly the same as the real dataset. Some of the low frequency tokens may not be included which leads to lower dimensionality, and the length of the synthetic data has changed. This is reflected on the time efficiency. The synthetics data does not have sequence anymore. Since Naive Bayes model already assumes no sequence in the document, the synthetic data does not negatively impact the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts (train): 0.9878888888888889\n",
      "raw_counts (test): 0.9895\n",
      "Time for raw counts section: 13.52 seconds\n",
      "tfidf (train): 0.9877777777777778\n",
      "tfidf (test): 0.9895\n",
      "Time for tfidf section: 4.31 seconds\n"
     ]
    }
   ],
   "source": [
    "nb.run_experiment(\"../data/synthetic_music.txt\", \"../data/synthetic_sports.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN Model on Synthetic Data\n",
    "In the case of the RNN applied to synthetic data, the qualitative evaluation reveals that the loss fluctuates, suggesting challenges in convergence or sensitivity to the dataset. Similarly, the accuracy exhibits a less clear trend, possibly indicating difficulties in learning patterns within the synthetic data. Notably, the training time for each epoch is shorter compared to the RNN on real data, likely attributed to the smaller dataset.\n",
    "Quantitatively, the final accuracy after 20 epochs is 61.55%, signifying moderate success in classifying synthetic data. The total training time is 92.18 seconds for 20 epochs, which is shorter than the RNN applied to real data. \n",
    "The fluctuating loss and accuracy trends imply potential challenges in learning patterns within the synthetic data. Consequently, further investigation into data quality and potential adjustments to hyperparameters, such as the learning rate, batch size, and hidden layer size, may be beneficial to enhance overall performance. Additionally, basic RNNs might face challenges in capturing long-term dependencies in sequences. Exploring more advanced architectures, such as LSTM, could potentially mitigate this limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read files done\n",
      "Start training\n",
      "Epoch 1/20, Loss: 0.6934, Accuracy: 0.5108, Time: 4.57 seconds\n",
      "Epoch 2/20, Loss: 0.6877, Accuracy: 0.5211, Time: 4.57 seconds\n",
      "Epoch 3/20, Loss: 0.6876, Accuracy: 0.5232, Time: 4.42 seconds\n",
      "Epoch 4/20, Loss: 0.6869, Accuracy: 0.5133, Time: 4.88 seconds\n",
      "Epoch 5/20, Loss: 0.6795, Accuracy: 0.5277, Time: 4.59 seconds\n",
      "Epoch 6/20, Loss: 0.6806, Accuracy: 0.5252, Time: 4.60 seconds\n",
      "Epoch 7/20, Loss: 0.6703, Accuracy: 0.5414, Time: 4.53 seconds\n",
      "Epoch 8/20, Loss: 0.6329, Accuracy: 0.6499, Time: 4.53 seconds\n",
      "Epoch 9/20, Loss: 0.6090, Accuracy: 0.6741, Time: 4.65 seconds\n",
      "Epoch 10/20, Loss: 0.6826, Accuracy: 0.5214, Time: 4.72 seconds\n",
      "Epoch 11/20, Loss: 0.6804, Accuracy: 0.5239, Time: 4.59 seconds\n",
      "Epoch 12/20, Loss: 0.6765, Accuracy: 0.5458, Time: 4.71 seconds\n",
      "Epoch 13/20, Loss: 0.6676, Accuracy: 0.5697, Time: 4.61 seconds\n",
      "Epoch 14/20, Loss: 0.6338, Accuracy: 0.6486, Time: 4.65 seconds\n",
      "Epoch 15/20, Loss: 0.5771, Accuracy: 0.7236, Time: 4.63 seconds\n",
      "Epoch 16/20, Loss: 0.6408, Accuracy: 0.6438, Time: 4.59 seconds\n",
      "Epoch 17/20, Loss: 0.6644, Accuracy: 0.6152, Time: 4.65 seconds\n",
      "Epoch 18/20, Loss: 0.6616, Accuracy: 0.6163, Time: 4.62 seconds\n",
      "Epoch 19/20, Loss: 0.6598, Accuracy: 0.6148, Time: 4.53 seconds\n",
      "Epoch 20/20, Loss: 0.6597, Accuracy: 0.6155, Time: 4.55 seconds\n",
      "Total training time: 92.18 seconds\n"
     ]
    }
   ],
   "source": [
    "rnn.train_rnn_model(\"../data/synthetic_music.txt\", \"../data/synthetic_sports.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both real data and synthetic data, Naive Bayes model has advantage over correctness and time efficiency. Usually RNNs perform better in classification since it takes into account the sequence of data. However, in our case, the documents are mainly tags, which barely have any sequence, so RNN does not have much advantages. We can potentially improve our Naive Bayes model by applying ensemble methods, such as bagging or boosting to combine multiple Naive Bayes models or combine Naive Bayes with other classifiers. We can also add smoothing to the zero probabilities in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding RNNs, in terms of quality and correctness, these models excel at capturing sequential dependencies in data, rendering them suitable for tasks where the order of video tags is crucial. However, for longer video tags, the model's ability to capture long-term dependencies diminishes. Additionally, RNNs possess limited memory, and extended sequences may result in information loss over time.\n",
    "In the realm of data, RNNs exhibit proficiency in handling diverse variable-length sequences, which proves advantageous for tasks involving varying sequence lengths. Moreover, RNNs can learn embeddings, enabling them to represent input data in a meaningful manner. Nevertheless, the performance of RNNs is heavily contingent on the quality and representativeness of the training data. Notably, as the synthetic data changes, the model's performance tends to worsen.\n",
    "In terms of training time, it is noteworthy that training RNNs requires a more extended duration compared to Naive Bayes. Concerning computational requirements, the process of training RNN architectures with a substantial number of parameters can be computationally demanding. Furthermore, RNNs may necessitate significant memory, particularly when dealing with lengthier sequences.\n",
    "Regarding interpretability, RNNs offer the advantage of allowing the interpretation of weights, providing insights into the crucial elements of the sequence for predictions. However, the model's complexity poses a challenge, making it difficult to interpret and comprehend internal representations effectively.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
